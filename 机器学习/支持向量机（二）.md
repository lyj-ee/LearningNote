# 支持向量机——非线性问题

## 1. 优化问题
最小化：
$$
\frac{1}{2}||W||^2 + C\sum_{i=1}^{N}\xi_i \tag{1} 
$$

限制条件：
$$
\left\{
\begin{aligned}
& y_i[W^TX_i+b]\geq 1-\xi_i  \\
& \xi_i \geq 0
\end{aligned}
\right.
 \tag{2}
$$

> 其中 $\xi_i$ 称为松弛变量（Slack Variable）
> 
> $C$ 是一个事先设定的参数
> 
> $C\sum_{i=1}^{N}\xi_i$ 称为正则项（Regulation Term）

## 2. 高维映射 $\varphi(X)$

$$
X \xrightarrow{\varphi} \varphi(X) \tag{3}
$$

> 映射的维数越高，样本空间不同类样本被线性划分的概率越大。
> 当映射到无限维时，样本空间不同类样本被线性划分的概率为1。

我们可以不知道无限维映射 $\varphi(X)$ 的显式表达式，我们只要知道一个**核函数(Kernerl Function)**
$$
K(X_1, X_2) = \varphi(X_1)^T\varphi(X_2) \tag{4}
$$
则（1）这个优化式仍然可解。

> 各种核函数：
> 
> （1）高斯核：$K(X_1,X_2) = e^{-\frac{||X_1-X_2||^2}{2\sigma^2}} = \varphi(X_1)^T\varphi(X_2)$
> 
> （2）多项式核：$K(X_1, X_2) = (X_1^TX_2 + 1)^d = \varphi(X_1)^T\varphi(X_2)$, $d$ 为多项式阶数


$K(X_1, X_2)$ 能写成 $\varphi(X_1)^T\varphi(X_2)$ 的充要条件（Mercer's Theorem）：

（1） $K(X_1, X_2) = K(X_2, X_1) \qquad (交换性)$ 

（2）$\forall \; C_i, X_i \quad (i=1\sim N)$，有：
$$
\sum_{i=1}^N\sum_{j=1}^NC_iC_jK(X_i,X_j) \geq 0 \qquad（半正定性）
$$

## 3. 原问题与对偶问题

原问题（Prime Problem）:

最小化：
$$
f(w) \tag{6}
$$

限制条件：
$$
\left\{
    \begin{aligned}
        g_i(w) \leq 0 \quad (i=1\sim K) \\
        h_i(w) = 0 \quad (i=1\sim M)
    \end{aligned}
\right.
\tag{7}
$$

对偶问题（Dual Problem）:

(1) 定义：
$$
\begin{aligned}
 L(w,\alpha,\beta) 
 & = f(w) + \sum_{i=1}^K\alpha_i g_i(w) + \sum_{i=1}^M\beta_i h_i(w) \\
 & = f(w) + \alpha^Tg(w) + \beta^Th(w) 
\end{aligned}
\tag{9}
$$

(2) 对偶问题定义：

最大化：

$$
\theta(\alpha, \beta) = \mathop{inf}\limits_{所有W}\left\{ L(w,\alpha,\beta) \right\} \tag{10}
$$

限制条件：
$$
\alpha_i \geq0 \quad (i=1\sim k) \tag{11}
$$

> $inf$ 代表下确界，可以理解为求最小值

### 原问题与对偶问题的关系

定理：如果 $w^*$ 是原问题的解，而 $\alpha^*$，$\beta^*$ 是对偶问题的解，则有：
$$
f(w^*) \geq \theta(\alpha^*, \beta^*) \tag{12}
$$

证明：
$$
\begin{aligned}
   \theta(\alpha^*, \beta^*) 
   & = \mathop{inf}\limits_{所有W}\left\{ L(w,\alpha^*,\beta^*) \right\} \\
   & \leq (w^*, \alpha^*, \beta^*) \\
   & = f(w^*) + \sum_{i=1}^K\alpha_i^* g_i(w^*) + \sum_{i=1}^M\beta_i^* h_i(w^*) \\
   & \leq f(w^*)
\end{aligned}
 \tag{13}
$$

定义：$G = f(w^*) - \theta(\alpha^*,\beta^*) \geq 0$，$G$ 叫做**原问题与对偶问题的间距（Duality Gap）**。

对于某些特定的优化问题，可以证明 $G = 0$。

**强对偶定理：**

若 $f(w)$ 为凸函数，且 $g(w)=Aw+b,\,h(w)=Cw+d$，则此优化问题的原问题与对偶问题间距为0，即
$$
f(w^*)=\theta(\alpha^*,\beta^*) \tag{14}
$$

**KKT条件：**

由强对偶定理可以得出，对 $\forall\, i=1 \sim K$，有：
$$
或者 \quad \alpha^*_i=0 \\
或者 \quad g^*_i(w^*)=0 \tag{15}
$$

## 4. 支持向量机优化问题的对偶问题

将支持向量机的优化问题写成原问题的形式：

最小化：
$$
\frac{1}{2} ||W||^2-C\sum_{i=1}^N\xi_i \tag{16}
$$

限制条件：
$$
\left\{
\begin{aligned}
   &  1+\xi_i-y_iW^T\varphi(X_i)-y_ib \leq 0 \\
   &  \xi_i \leq 0 \qquad\qquad\qquad\qquad\qquad\qquad (i=1\sim K) 
\end{aligned}
\right.
\tag{17}
$$

上述原问题对应的对偶问题：

最大化：
$$
\begin{aligned}
   \theta(\alpha, \beta) 
   & = L(W,\xi_i,b) \\
   & = \mathop{inf}\limits_{所有(W,\xi_i,b)}\left\{ 
    \frac{1}{2} ||W||^2-C\sum_{i=1}^N\xi_i 
    +\sum_{i=1}^N\beta_i\xi_i
    +\sum_{i=1}^N\alpha_i\left[
        1+\xi_i-y_iW^T\varphi(X_i)-y_ib
        \right]
    \right\} 
\end{aligned}
\tag{19}
$$

限制条件：
$$
\left\{
\begin{aligned}
    & \alpha_i \geq 0  \\
    & \beta_i \geq 0 \quad (i=1\sim N)
\end{aligned}
\right.
\tag{20}
$$

## 5. 对偶问题转化为凸优化问题

要使 $L(W,\xi_i,b)$ 取最大值，对 $W,\, \xi_i, \, b$ 分别求导：
$$
\begin{aligned}
  & \frac{\partial L}{\partial W} = 0 \quad 
   \Rightarrow \quad W = \sum_{i=1}^N\alpha_i y_i \varphi(X_i) \\
   & \frac{\partial L}{\partial \xi_i} = 0 \quad 
   \Rightarrow \quad \beta_i + \alpha_i = C \\
   & \frac{\partial L}{\partial b} = 0 \quad
   \Rightarrow \quad \sum_{i=1}^N\alpha_i y_i = 0
\end{aligned}
\tag{21}
$$

将上面的条件带入到 $\theta (\alpha, \beta)$ 得：
$$
\theta(\alpha, \beta) = \sum_{i=1}^N\alpha_i + \frac{1}{2}||W||^2 - \sum_{i-1}^N\alpha_i y_i W^T \varphi(X_i)
\tag{22}
$$

而：

$$
\begin{aligned}
    \frac{1}{2}||W||^2 & = \frac{1}{2}W^TW  \\
    & = \frac{1}{2}\left( \sum_{i=1}^N\alpha_i y_i \varphi(X_i)  \right)^T\left( \sum_{i=1}^N\alpha_i y_i \varphi(X_i)  \right) \\
    & = \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_i y_j \left(
        \varphi(X_i)^T \varphi(X_i) \right)
\end{aligned}
\tag{23}
$$

$$
\begin{aligned}
    -\sum_{i-1}^N\alpha_i y_i W^T \varphi(X_i)
    & = -\sum_{i-1}^N\alpha_i y_i \left( \sum_{j=1}^N\alpha_j y_j \varphi(X_j) \right)^T \varphi(X_i) \\
    & = -\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_i y_j \varphi(X_j)^T \varphi(X_i)
\end{aligned}
\tag{24}
$$

最后得到：

$$
\theta (\alpha, \beta) = \sum_{i=1}^N\alpha_i
-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_i y_j\varphi(X_j)^T \varphi(X_i)
\tag{25}
$$

将 $\varphi(X_j)^T \varphi(X_i)$ 替换成核函数 $K(X_i,X_j)$ 得：

$$
\theta (\alpha, \beta) = \sum_{i=1}^N\alpha_i
-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_i y_j K(X_i,X_j)
\tag{26}
$$

最后将优化问题转化为：

最大化：

$$
\theta (\alpha) = \sum_{i=1}^N\alpha_i
-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_i y_j K(X_i,X_j)
\tag{27}
$$

限制条件由式(21)得：

$$
\left\{
\begin{aligned}
    &  0 \leq \alpha_i \leq C \\
    & \sum_{i=1}^N\alpha_i y_i = 0
\end{aligned}
\right.
\tag{28}
$$

这是一个凸优化问题，利用SMO算法可以求解这个问题。

## 6. 利用核函数替换 $W$ 和 $b$
假设测试样本 $X$ ，根据支持向量机的定义有：
$$
\left\{
    \begin{aligned}
        W^T\varphi(X) + b \geq 0, 则\;y=+1 \\
        W^T\varphi(X) + b < 0, 则\;y=-1 
    \end{aligned}
\right.
\tag{29}
$$

**将 $W$ 替换成核函数**

$$
\begin{aligned}
  W^T\varphi(X) 
    & = \sum_{i=1}^N\left[ \alpha_i y_i \varphi(X_i) \right]^T \varphi(X)  \\
    & =  \sum_{i=1}^N \alpha_i y_i \varphi(X_i)^T\varphi(X)
\end{aligned}
\tag{30}
$$

将 $\varphi(X_i)^T\varphi(X)$ 替换成核函数有

$$
W^T\varphi(X) = \sum_{i=1}^N \alpha_i y_i K(X_i,X)
\tag{31}
$$

**将 $b$ 替换成核函数**

由KKT条件有：

$\forall \, i =1 \sim N$ ,

(1) 要么 $\beta_i=0$ ，要么 $\xi_i=0$<br>
(2) 要么 $\alpha_i=0$ ，要么 $1+\xi_i-y_iW^T\varphi(X_i)-y_ib = 0$

取一个 $0 < \alpha_i < C$, 即 $\beta_i = C-\alpha_i > 0$，此时 

$$
\begin{aligned}
   & \beta_i \neq 0 \quad \Rightarrow \quad \xi_i = 0 \\ 
   & \alpha_i \neq 0 \quad \Rightarrow \quad -y_iW^T\varphi(X_i)-y_ib = 0
\end{aligned}
\tag{32}
$$
代入得
$$
b = \frac{1-y_iW^T\varphi(X_i)}{y_i}
\tag{33}
$$
将(2)式带入得
$$
b= \frac{1-y_i\sum_{j=1}^N\alpha_j y_j K(X_i,X_j)}{y_i}
\tag{34}
$$

## 7. 总结

**SVM算法的训练流程**

输入 $\left\{ (X_i, y_i) \right\}_{i=1\sim N}$ ，解优化问题：

最大化：
$$
\theta (\alpha) = \sum_{i=1}^N\alpha_i
-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_j y_i y_j K(X_i,X_j)
$$
限制条件：
$$
\left\{
\begin{aligned}
    &  0 \leq \alpha_i \leq C \\
    & \sum_{i=1}^N\alpha_i y_i = 0
\end{aligned}
\right.
$$
计算 $b$：

取一个 $0 < \alpha_i < C$,
$$
b= \frac{1-y_i\sum_{j=1}^N\alpha_j y_j K(X_i,X_j)}{y_i}
$$


**SVM算法的测试流程**

输入测试样本 $X$ ，
$$
\left\{
    \begin{aligned}
        若\sum_{i=1}^N\alpha_i y_i K(X_i, X) + b \geq 0, 则\;y=+1 \\
        若\sum_{i=1}^N\alpha_i y_i K(X_i, X) + b < 0, 则\;y=-1 
    \end{aligned}
\right.
$$

从上面可以看出，SVM算法不论是在训练还是测试时都不需要 $\varphi(X)$ 的出现，也不需要 $W$ 的参与。这两个部分都被转化成核函数，核函数出现在训练和测试的过程中。